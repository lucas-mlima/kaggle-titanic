{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition\n",
    "\n",
    "# Titanic: Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The challenger\n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "random.seed(0)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    df_train = pd.read_csv('D:\\\\Kaggle\\\\Titanic\\\\train.csv')\n",
    "    print('File 1 loading - Success!')\n",
    "    df_test = pd.read_csv('D:\\\\Kaggle\\\\Titanic\\\\test.csv')\n",
    "    print('File 2 loading - Success!')\n",
    "except:\n",
    "    print('File loading - Failed!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analyses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "* PassengerId: Unique passenger identification.\n",
    "* Survived: Whether a passenger survived or not; 1 if survived and 0 if not.\n",
    "* Pclass: Ticket class; 1 = 1st, 2 = 2nd, 3 = 3rd.\n",
    "* Name: Passanger name.\n",
    "* Sex: Passanger gender. \n",
    "* Age: Passanger age in years.\n",
    "* SibSp: Number of sibling/spouses aboard the Titanic.\n",
    "* Parch: Number of children/parents aboard the Titanic.\n",
    "* Ticket: Ticket number.\n",
    "* Fare: Passanger fare.\n",
    "* Cabin: Cabin number. \n",
    "* Embarked: Port of embarkation; C = Cherbourg, Q = Queenstown, S = Southampton.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we can see that:\n",
    "\n",
    "* Age feature has missing values;\n",
    "* There are indications of non-normalities of some features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(\"Survived\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "* We have a lot of missing values in 2 features ('Age' and 'Cabin'). The 'Cabin' feature will be discarded due to the large number of NaNs, but its informative content can generate useful information about the respective deck. About 'Age', your NaNs will be studied in the topic \"Missing Values\".\n",
    "\n",
    "* The features 'PassengerId' and 'Name' will be discarded, because they are variables of individual identification. However, before discarding 'Name' we will create a new feature based on the individual's treatment pronouns, which can be important information.\n",
    "\n",
    "* NaNs are also present in the test data, the treatment given to the training data will also be given to the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look in our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the dependent variables in relation to the variable of interest.\n",
    "\n",
    "for i in ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']: \n",
    "    plot = sns.FacetGrid(df_train, col='Survived')\n",
    "    plot.map(plt.hist, i, bins=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "* We can observe an imbalance of some variables, such as 'Sex' which has more women than men or 'Embarked' where the majority of the embarks were in Cherbourg ('C').\n",
    "\n",
    "* We can think of the first questions, such as:\n",
    "    1. Does gender matter? Does being a woman increase my chances of surviving?\n",
    "    2. Does ticket class matter?\n",
    "    3. Does boarding Southamptom increase my chances of survival? ('Embarked' = 'S')\n",
    "    \n",
    "    \n",
    "In the end, the goal is to know who survives or who doesn't.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step:\n",
    "    \n",
    "    1. Combine Features;\n",
    "    2. Missing values;\n",
    "    3. Transform features;\n",
    "    4. Scaling numerical data;\n",
    "    5. Drop features;\n",
    "    6. Dummies;\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "Initially, we have two features with informational content that can be combined, which are 'SibSp' and 'Parch'. If we combine, we can build features called 'Family_size' and 'Travelled_alone'. \n",
    "\n",
    "* 'Family_size': is the sum of 'SibSp' and 'Parch' plus 1, indicating the size of the family.\n",
    "\n",
    "* 'Travelled_alone': contains the information whether the individual travelled alone or not (given the domain of the original features).\n",
    "\n",
    "Given the new feature 'Familyy_size', we can combine it with 'Fare' and find out what the average fare for the individual's family group is.\n",
    "\n",
    "* 'Fare_per_family': the average fare for the individual's family group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Family_size'] = df_train['SibSp'] + df_train['Parch'] + 1\n",
    "df_test['Family_size'] = df_test['SibSp'] + df_test['Parch'] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare per family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Fare_per_family'] = df_train['Fare'] / df_train['Family_size']\n",
    "df_test['Fare_per_family'] = df_test['Fare'] / df_test['Family_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travelled alone    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create 'Travelled_alone' feature\n",
    "\n",
    "def feature_travelled_alone(data):\n",
    "    \n",
    "    data['Travelled_alone'] = \"\" #creates an empty feature called 'Travelled_alone' in the dataset \n",
    "        \n",
    "    data.loc[ (data['SibSp'] == 0) & (data['Parch'] == 0), 'Travelled_alone'] = 1 #condition to travelled alone.\n",
    "    data.loc[ (data['SibSp'] != 0) | (data['Parch'] != 0), 'Travelled_alone'] = 0\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = feature_travelled_alone(df_train)\n",
    "df_test = feature_travelled_alone(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "* As stated earlier, we will investigate 'Age' and 'Cabin' NaNs.\n",
    "\n",
    "About 'Cabin', owning one was not for everyone, possibly individuals with NaNs just didn't have a cabin. In this way, we can create a feature containing the information whether or not it has a cabin and/or the deck's correspondence (which is information contained in the cabin identification, like 'C' or 'A').\n",
    "\n",
    "The first question about 'Age':\n",
    "\n",
    "* Do 'Age' NaNs have a pattern or are they random?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[ (df_train['Age'].isnull() == True) ] #data with NaNs in 'Age', looking for some pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[ (df_train['Age'].isnull() == True) ].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in ['Pclass','Sex','Embarked','Travelled_alone']: \n",
    "    plot = sns.FacetGrid(df_train[ (df_train['Age'].isnull() == True) ], col='Survived')\n",
    "    plot.map(plt.hist, i, bins=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "* Looking at the outputs above, we have indications that the 'Age' NaNs are not random. Thus, the simple disposal of these lines with NaN cannot be done.\n",
    "\n",
    "* Group indication (possible): 'Pclass' = 3, 'Embarked' = 'Q' and 'Travelled_alone' = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupy_NaN = df_train[ (df_train['Pclass'] == 3) ]\n",
    "df_groupy_NaN.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupy_NaN = df_train[ (df_train['Embarked'] == 'Q') ]\n",
    "df_groupy_NaN.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupy_NaN = df_train[ (df_train['Travelled_alone'] == 1) ]\n",
    "df_groupy_NaN.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupy_NaN = df_train[ (df_train['Pclass'] == 3) ]\n",
    "df_groupy_NaN = df_groupy_NaN[ (df_groupy_NaN['Travelled_alone'] == 1) ]\n",
    "#df_groupy_NaN = df_groupy_NaN[ (df_groupy_NaN['Embarked'] == 'Q') ]\n",
    "df_groupy_NaN.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupy_NaN = df_train[ (df_train['Pclass'] == 3) ]\n",
    "#df_groupy_NaN = df_groupy_NaN[ (df_groupy_NaN['Travelled_alone'] == 1) ]\n",
    "df_groupy_NaN = df_groupy_NaN[ (df_groupy_NaN['Embarked'] == 'Q') ]\n",
    "df_groupy_NaN.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupy_NaN = df_train[ (df_train['Pclass'] == 3) ]\n",
    "df_groupy_NaN = df_groupy_NaN[ (df_groupy_NaN['Travelled_alone'] == 1) ]\n",
    "df_groupy_NaN = df_groupy_NaN[ (df_groupy_NaN['Embarked'] == 'Q') ]\n",
    "df_groupy_NaN.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "* This specific group (using the 3 equalities) corresponds to 20% of the total 'Age' NaNs in training data.\n",
    "\n",
    "* Filtering by 'Pclass' and 'Travelled_alone' we have 56% of the total 'Age' NaNs in training data. This will be our specific group.\n",
    "\n",
    "* The treatment of NaNs will be done based on the group for those who belong to the identified group. The others will be treated considering the complete sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to imput data in 'Age' considering specific group \n",
    "\n",
    "\n",
    "# We will imput random values based on the mean and standard deviation of the data (total or group)\n",
    "\n",
    "\n",
    "\n",
    "def imput_data_by_group(data_1, data_2):\n",
    "    \n",
    "    data = data_1.append(data_2, sort = False) #merge training and test data\n",
    "    \n",
    "    data_group = data[ (data['Pclass'] == 3) ] #first group filter\n",
    "    data_group = data_group[ (data_group['Travelled_alone'] == 1) ] #second group filter\n",
    "    \n",
    "    mean_base = data['Age'].mean() #average of full data \n",
    "    std_base = data['Age'].std() #standard desviation of full data\n",
    "    \n",
    "    mean_group = data_group['Age'].mean() #average of group on full data\n",
    "    std_group = data_group['Age'].std() #standard desviation of group on full data\n",
    "    \n",
    "    \n",
    "    for i in range(len(data_1)): #treating NaNs from training data \n",
    "        if (pd.isnull(data_1.loc[i,'Age']) == True):\n",
    "            if (data_1.loc[i,\"Pclass\"] == 3) and (data_1.loc[i,\"Travelled_alone\"] == 1): #condition to be part of the group\n",
    "                data_1.loc[i,'Age'] = np.random.randint(mean_group - std_group, mean_group + std_group, 1) \n",
    "                #imputing random value\n",
    "            else:\n",
    "                data_1.loc[i,'Age'] = np.random.randint(mean_base - std_base, mean_base + std_base, 1)\n",
    "                #imputing random value\n",
    "                \n",
    "    for i in range(len(data_2)): #treating NaNs from test data \n",
    "        if (pd.isnull(data_2.loc[i,'Age']) == True):\n",
    "            if (data_2.loc[i,\"Pclass\"] == 3) and (data_2.loc[i,\"Travelled_alone\"] == 1): #condition to be part of the group\n",
    "                data_2.loc[i,'Age'] = np.random.randint(mean_group - std_group, mean_group + std_group, 1)\n",
    "                #imputing random value\n",
    "            else:\n",
    "                data_2.loc[i,'Age'] = np.random.randint(mean_base - std_base, mean_base + std_base, 1)\n",
    "                #imputing random value\n",
    "    \n",
    "    return (data_1, data_2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "\n",
    "df_train, df_test = imput_data_by_group(df_train, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[ df_train['Embarked'].isnull() == False]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "* As we cannot rule out any observation from the test base, we will imput based on the average and standard deviation, in the same way as we did for 'Age'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to imput data in 'Fare' and 'Fare_per_family'\n",
    "\n",
    "\n",
    "def imput_data_fare(data_1, data_2, columns):\n",
    "    \n",
    "    data = data_1.append(data_2, sort = False) #merge training and test data\n",
    "    \n",
    "    mean_base = data[columns].mean() #average of full data \n",
    "    std_base = data[columns].std() #standard desviation of full data\n",
    "                \n",
    "    for i in range(len(data_2)): #treating NaNs from test data \n",
    "        if (pd.isnull(data_2.loc[i,columns]) == True):\n",
    "                data_2.loc[i,columns] = np.random.randint(mean_base - std_base, mean_base + std_base, 1) #imputing random value\n",
    "    \n",
    "    return (data_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "df_test = imput_data_fare(df_train, df_test,'Fare')\n",
    "df_test = imput_data_fare(df_train, df_test,'Fare_per_family')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'Age' - We can transform the numeric variable 'Age' into a new categorical feature ('Age_group'), depending on age groups. Being, respectively: child; teen; young adult; adult; senior; and, retired.\n",
    "\n",
    "'Fare' - We can transform the numeric variable 'Fare' into a new categorical feature ('Fare_group'), depending on fare groups. Being, respectively: very low; low; base; high; and,  very high.\n",
    "\n",
    "'Name' - We can extract the pronoun from the individual's treatment by removing this information from the 'Name' resource and creating a new categorical resource called 'Title'.\n",
    "\n",
    "'Cabin' - As stated earlier, we are going to create a new feature called 'Deck' based on the information contained in the 'Cabin' feature. For individuals without this information, we will assign the string 'U' in reference to 'Unknown'.\n",
    "\n",
    "'Pclass' - We will change the type of the feature value to string, to facilitate the process of making it a dummy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a feature 'Age_group' (categorial feature) \n",
    "\n",
    "\n",
    "def convert_age_to_group(data):\n",
    "    \n",
    "    data['Age_group'] = ''\n",
    "    \n",
    "    data.loc[ data['Age'] <= 12, 'Age_group'] = 'child'\n",
    "    data.loc[(data['Age'] > 12) & (data['Age'] <= 18), 'Age_group'] = 'teen'\n",
    "    data.loc[(data['Age'] > 18) & (data['Age'] <= 27), 'Age_group'] = 'young_adult'\n",
    "    data.loc[(data['Age'] > 27) & (data['Age'] <= 40), 'Age_group'] = 'adult'\n",
    "    data.loc[(data['Age'] > 40) & (data['Age'] <= 59), 'Age_group'] = 'senior'\n",
    "    data.loc[(data['Age'] > 59), 'Age_group'] = 'retired'\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = convert_age_to_group(df_train)\n",
    "df_test = convert_age_to_group(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Age_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a feature 'Fare_rate' (categorial feature) \n",
    "\n",
    "def convert_fare_to_group(data):\n",
    "    \n",
    "    data['Fare_rate'] = ''\n",
    "    \n",
    "    data.loc[ data['Fare'] <= 8, 'Fare_rate'] = 'very_low'\n",
    "    data.loc[(data['Fare'] > 8) & (data['Fare'] <= 16), 'Fare_rate'] = 'low'\n",
    "    data.loc[(data['Fare'] > 15) & (data['Fare'] <= 32), 'Fare_rate'] = 'base'\n",
    "    data.loc[(data['Fare'] > 32) & (data['Fare'] <= 64), 'Fare_rate'] = 'high'\n",
    "    data.loc[(data['Fare'] > 64), 'Fare_rate'] = 'very_high'\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = convert_fare_to_group(df_train)\n",
    "df_test = convert_fare_to_group(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Fare_rate'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a feature 'Title' (categorial feature) \n",
    "\n",
    "\n",
    "def convert_title_to_group(data):\n",
    "    \n",
    "    data['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "    data.loc[ (data['Title'] == 'Ms'), 'Title'] = 'Miss'\n",
    "    data.loc[ (data['Title'] == 'Mlle'), 'Title'] = 'Miss' \n",
    "    data.loc[ (data['Title'] == 'Mme'), 'Title'] = 'Mrs'\n",
    "    \n",
    "    data.loc[ (data['Title'] != 'Mr') & (data['Title'] != 'Mrs') & (data['Title'] != 'Miss') & (data['Title'] != 'Master'), 'Title'] = 'Distinct'           \n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = convert_title_to_group(df_train)\n",
    "df_test = convert_title_to_group(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pclass to categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Pclass'] = df_train['Pclass'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a feature 'Deck' (categorial feature) \n",
    "\n",
    "def convert_cabin_to_deck(data):\n",
    "    \n",
    "    data['Deck'] = data['Cabin'].fillna('U0')\n",
    "    data['Deck'] = [x[0] for x in data['Deck'].values]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = convert_cabin_to_deck(df_train)\n",
    "df_test = convert_cabin_to_deck(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Deck'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "Because of the low variability of the data, we will not use the features of deck A, F, G and T after creating your dummies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform 'Cabin' into new  feature\n",
    "\n",
    "def convert_cabin_to_havecabin(data):\n",
    "    \n",
    "    data['Have_cabin'] = ''\n",
    "    \n",
    "    data.loc[(data['Cabin'].isna() == False ), 'Have_cabin'] = 1 #it is a dummy features, 1 have and 0 don't have cabin\n",
    "    data.loc[(data['Cabin'].isna() == True ), 'Have_cabin'] =  0\n",
    "\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = convert_cabin_to_havecabin(df_train)\n",
    "df_test = convert_cabin_to_havecabin(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "Discarding the features previously indicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "    \n",
    "As I will use dummy variables (0 or 1), I chose to use the MinMax function, returning the numerical values on the scale between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = list(df_train.select_dtypes(include=['int64', 'float64', 'int32']).columns)[1:-2] #\n",
    "\n",
    "#ss_scaler = StandardScaler()\n",
    "\n",
    "ss_scaler = MinMaxScaler()\n",
    "\n",
    "df_train = pd.DataFrame(data = df_train)\n",
    "df_train[num_features] = ss_scaler.fit_transform(df_train[num_features])\n",
    "\n",
    "df_test = pd.DataFrame(data = df_test)\n",
    "df_test[num_features] = ss_scaler.fit_transform(df_test[num_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "Remember the dummy variable rule (n - 1). We will adapt to this rule later on, at the moment we will keep all the dummy features created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = list(df_train.select_dtypes(include=['object']).columns) #categorical features to change into dummies\n",
    "\n",
    "\n",
    "#transforming categorical data into dummy features\n",
    "\n",
    "for i in cat_features: #for training data\n",
    "    df_train = pd.concat([df_train, pd.get_dummies(df_train[i], prefix=i)], axis=1)\n",
    "    df_train.drop(i, axis = 1, inplace=True)\n",
    "    \n",
    "    \n",
    "for i in cat_features: #for test data\n",
    "    df_test = pd.concat([df_test, pd.get_dummies(df_test[i], prefix=i)], axis=1)\n",
    "    df_test.drop(i, axis = 1, inplace=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data snapshot after pre-procesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(16, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "We can observe a moderate correlation between 'Suviver' and 'Sex_female', it is a good insight into who may have survived. The expected correlations for the other features were observed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapting the features given the dummy rule\n",
    "\n",
    "\n",
    "features = ['Survived', 'Parch', 'Fare', 'Family_size', 'Travelled_alone', 'Fare_per_family', 'Pclass_1', 'Pclass_2', \n",
    "            'Sex_female', 'Embarked_C', 'Embarked_Q', 'Age_group_child', 'Age_group_teen', 'Age_group_adult', 'Age_group_young_adult', \n",
    "            'Age_group_senior', 'Fare_rate_very_high','Fare_rate_high', 'Fare_rate_base','Fare_rate_low', 'Title_Master', 'Title_Miss',\n",
    "            'Title_Mr', 'Title_Mrs','Deck_A','Deck_B','Deck_C','Deck_D','Deck_E','Deck_U']\n",
    "\n",
    "\n",
    "\n",
    "#drop: 'PassengerId', 'Name', 'Age', SibSp','Ticket', 'Cabin', 'Have_cabin','Embarked_S', 'Age_group_senior', 'Title_Distinct', 'Fare_rate_very_high',\n",
    "#   'Pclass_3','Deck_F','Deck_G' and 'Deck_T'. \n",
    "\n",
    "\n",
    "\n",
    "features_dummy = ['Survived', 'Travelled_alone', 'Have_Cabin', 'Pclass_1', 'Pclass_2', \n",
    "            'Pclass_3', 'Sex_female','Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Age_group_child', 'Age_group_teen', \n",
    "            'Age_group_adult', 'Age_group_young_adult', 'Age_group_senior', 'Fare_rate_very_high','Fare_rate_high', 'Fare_rate_base',\n",
    "            'Fare_rate_low', 'Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs','Deck_A','Deck_B','Deck_C','Deck_D','Deck_E','Deck_U']\n",
    "\n",
    "\n",
    "\n",
    "features_all = ['Survived', 'Age', 'SibSp','Parch', 'Fare', 'Family_size', 'Travelled_alone', 'Fare_per_family', 'Pclass_1', 'Pclass_2', \n",
    "            'Pclass_3', 'Sex_female','Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Age_group_child', 'Age_group_teen', 'Have_cabin',\n",
    "            'Age_group_adult', 'Age_group_young_adult', 'Age_group_senior', 'Fare_rate_very_high','Fare_rate_high', 'Fare_rate_base',\n",
    "            'Fare_rate_low', 'Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs','Deck_A','Deck_B','Deck_C','Deck_D','Deck_E','Deck_U']\n",
    "\n",
    "#drop: 'PassengerId', 'Name', 'Ticket', 'Cabin'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop deck, variavel ruim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "Which method to use?\n",
    "\n",
    "Basically, we will use here all the classification methods available from the sklearn library, but those that are compatible with crossvalidation and my data (Ex: the Gaussian Naive Bayes is for categorical data, and we also have numerical variables). Which, by their groups, are:\n",
    "\n",
    "\n",
    "* Discriminant Analysis:\n",
    "    1. Linear Discriminant Analysis;\n",
    "    2. Quadratic Discriminant Analysis.\n",
    "\n",
    "\n",
    "* Ensemble:\n",
    "    1. Ada Boost Classifier;\n",
    "    2. Bagging Classifier;\n",
    "    3. Extra Tree Classifier;\n",
    "    4. Gradient Boosting Classifier;\n",
    "    5. Ramdom Forest Classifier.\n",
    "\n",
    "\n",
    "* GLM:\n",
    "    1. Logistic;\n",
    "    2. Passive Agressive Classifier;\n",
    "    3. Perceptron;\n",
    "    4. Ridge Classifier;\n",
    "    5. SGD Classifier.\n",
    "\n",
    "\n",
    "* Naive Bayes:\n",
    "    1. Bernoulli Naive Bayes. \n",
    "\n",
    "\n",
    "* Nearest Neighbors:\n",
    "    1. K Neighbors Classifier.\n",
    "\n",
    "\n",
    "* NN:\n",
    "    1. Multi-layer Perceptron Classifier.\n",
    "\n",
    "\n",
    "* SVM:\n",
    "    1. SVC;\n",
    "    2. Nu-SVC;\n",
    "    3. Linear SVC.\n",
    "\n",
    "\n",
    "* Decision Trees:\n",
    "    1. Decision Tree Classifier;\n",
    "    2. Extra Tree Classifier.\n",
    "\n",
    "See: https://scikit-learn.org/stable/modules/classes.html# \n",
    "\n",
    "\n",
    "Additionally, we will also use the XGBoost method\n",
    "   \n",
    "   * Extreme Gradiente Boost. \n",
    "\n",
    "See: https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "\n",
    "\n",
    "\n",
    "Why use all of these classification methods?\n",
    "\n",
    "Because the purpose of the challenge and the nature of the data allow me to do that.\n",
    "\n",
    "\n",
    "Regarding the first statement, the challenge is whether those who survived or not survived, without going through an analysis within the sample or its dependent variables. If we needed to investigate the effects and significance of these input features, we would be restricted to parametric methods, with most of the methods used later being non-parametric. \n",
    "\n",
    "About the second statement, as noted in the correlogram earlier, if we obey the rule of dummies and avoid using the variables with high collineraryity, we will not have a restriction of methods.\n",
    "\n",
    "\n",
    "\n",
    "## Metrics\n",
    "    \n",
    "    \n",
    "As we will use cross validation, we will use average accuracy.\n",
    "\n",
    "\n",
    "## Crossvalidation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "See: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, tree, neighbors, naive_bayes, ensemble, linear_model, discriminant_analysis, gaussian_process\n",
    "from sklearn import model_selection, metrics\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected model will be based on the best average accuracy observed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_train, Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[features[1:]]\n",
    "Y_train = df_train[features[0]]\n",
    "\n",
    "\n",
    "X_test = df_test[features[1:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "\n",
    "methods = [\n",
    "    \n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(), \n",
    "    #https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    #https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    #https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    #https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    #https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    #https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "    #https://scikit-learn.org/stable/modules/classes.html#module-sklearn.discriminant_analysis\n",
    "    \n",
    "    #xgboost\n",
    "    XGBClassifier()    \n",
    "    #https://xgboost.readthedocs.io/en/latest/tutorials/index.html\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "#split dataset in cross-validation with this splitter class\n",
    "#note: this is an alternative to train_test_split\n",
    "\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0) \n",
    "\n",
    "\n",
    "#create table to compare parameters and metrics\n",
    "methods_columns = ['Method', 'Parameters', 'Test_Accuracy_Mean','Test_Accuracy_Std','Time']\n",
    "methods_compare = pd.DataFrame(columns = methods_columns)\n",
    "\n",
    "\n",
    "#create table to compare\n",
    "method_predict = pd.DataFrame(df_test['PassengerId'])\n",
    "\n",
    "\n",
    "#index through methods and save performance to table\n",
    "row_index = 0\n",
    "\n",
    "for method in methods:\n",
    "\n",
    "    #set name and parameters\n",
    "    method_name = method.__class__.__name__\n",
    "    methods_compare.loc[row_index, 'Method'] = method_name\n",
    "    methods_compare.loc[row_index, 'Parameters'] = str(method.get_params())\n",
    "    \n",
    "    #score model with cross validation: \n",
    "    cv_results = model_selection.cross_validate(method, X_train, Y_train, cv  = cv_split)\n",
    "\n",
    "    methods_compare.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n",
    "    methods_compare.loc[row_index, 'Test_Accuracy_Mean'] = cv_results['test_score'].mean()\n",
    "    methods_compare.loc[row_index, 'Test_Accuracy_Std'] = cv_results['test_score'].std()  \n",
    "\n",
    "    method.fit(X_train, Y_train)\n",
    "    method_predict[method_name] = method.predict(X_test)\n",
    "    \n",
    "    row_index += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_compare.sort_values(by = ['Test_Accuracy_Mean'], ascending = False, inplace = True)\n",
    "methods_compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_compare.to_csv('methods_compare_1.csv', index = False, encoding='utf-8')                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#result_gb_cv = pd.concat([pd.DataFrame(df_test['PassengerId'])], axis=1)\n",
    "#result_gb_cv['Survived'] = method_predict['GradientBoostingClassifier']\n",
    "\n",
    "#result_logit_cv = pd.concat([pd.DataFrame(df_test['PassengerId'])], axis=1)\n",
    "#result_logit_cv['Survived'] = method_predict['LogisticRegressionCV']                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_logit_cv.to_csv('result_logit_cv.csv', index = False, encoding='utf-8')\n",
    "\n",
    "#result_gb_cv.to_csv('result_gb_cv.csv', index = False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_lda_cv = pd.concat([pd.DataFrame(df_test['PassengerId'])], axis=1)\n",
    "#result_lda_cv['Survived'] = method_predict['LinearDiscriminantAnalysis']\n",
    "\n",
    "#result_lda_cv.to_csv('result_lda_cv.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_lsvc_cv = pd.concat([pd.DataFrame(df_test['PassengerId'])], axis=1)\n",
    "#result_lsvc_cv['Survived'] = method_predict['LinearSVC']\n",
    "\n",
    "#result_lsvc_cv.to_csv('result_lsvc_cv.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we improve performance?\n",
    "\n",
    "We can use 2 approaches to try to improve the performance of the models, which are the 'feature selection' and the 'tuning parameters'.\n",
    "\n",
    "In the 'feature selection' we use methods to select the features that will be used in the model, we will not use all those maintained in X_train and X_test. This selection aims to maintain only the features with the greatest predictive potential. \n",
    "\n",
    "The 'tuning parameters' is to select the arguments of the respective ones, selecting the group of arguments that obtained the best results based on a specific metric.\n",
    "  \n",
    "\n",
    "Here we will only use 'tunning parameters' to try to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use GridSearchCV to find the best hyperparameters, given the set of possibilities passed for each method.\n",
    "\n",
    "\n",
    "See: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote = [\n",
    "    \n",
    "    #Ensemble Methods:\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc', ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Gaussian Processes:\n",
    "    #('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #GLM: \n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    #('rr', linear_model.RidgeClassifierCV()),\n",
    "    \n",
    "    #Navies Bayes: \n",
    "    #('bnb', naive_bayes.BernoulliNB()),\n",
    "    #('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Nearest Neighbor: \n",
    "    #('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #SVM: \n",
    "    ('lsvc', svm.LinearSVC()),\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    ('lda', discriminant_analysis.LinearDiscriminantAnalysis()),\n",
    "    \n",
    "    #xgboost:\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid hyperparameter\n",
    "\n",
    "grid_n_estimator = [10, 50, 100, 300]\n",
    "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "grid_learn = [.01, .03, .05, .1, .25]\n",
    "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "grid_min_samples = [5, 10, .03, .05, .10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "\n",
    "grid_param = [\n",
    "    \n",
    "            [{\n",
    "            #AdaBoostClassifier\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'n_estimators': grid_n_estimator, #default=50\n",
    "            'learning_rate': grid_learn, #default=1\n",
    "            'random_state': grid_seed\n",
    "            #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n",
    "            }],\n",
    "       \n",
    "    \n",
    "            [{\n",
    "            #BaggingClassifier\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'max_samples': grid_ratio, #default=1.0\n",
    "            'bootstrap' : grid_bool, #default=True\n",
    "            'random_state': grid_seed\n",
    "            #'bootstrap_features' : boolean, optional (default=False)\n",
    "            #'oob_score' : bool, optional (default=False)\n",
    "            #'n_jobs' : int or None, optional (default=None)\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #ExtraTreesClassifier\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=”gini”\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            'bootstrap': grid_bool, #default=False\n",
    "            'random_state': grid_seed\n",
    "            #\n",
    "             }],\n",
    "\n",
    "\n",
    "            [{\n",
    "            #GradientBoostingClassifier\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            'loss': ['deviance', 'exponential'], #default=’deviance’\n",
    "            'learning_rate': grid_learn, #default=0.1 \n",
    "            'n_estimators': grid_n_estimator, #default=100 \n",
    "            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n",
    "            #'subsample' : , float, optional (default=1.0)    \n",
    "            'max_depth': grid_max_depth, #default=3   \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #RandomForestClassifier\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'n_estimators': grid_n_estimator, #default=10\n",
    "            'criterion': grid_criterion, #default=”gini”\n",
    "            'max_depth': grid_max_depth, #default=None\n",
    "            #'bootstrap': grid_bool, #default=True\n",
    "            'oob_score': [True,False], #default=False \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "        \n",
    "    \n",
    "            [{\n",
    "            #LogisticRegressionCV\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            #'C' : [5,10,20], #default: 10\n",
    "            'fit_intercept': grid_bool, #default: True\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n",
    "            'random_state': grid_seed\n",
    "            #'penalty': ['l1','l2'],\n",
    "             }],\n",
    "    \n",
    "            \n",
    "            #[{\n",
    "            #RidgeClassifier\n",
    "            #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier\n",
    "            #'alpha': [.1,.25,.5,.75,1], #default : 1\n",
    "            #'solver': ['auto','svd','cholesky','lsqr','sparce_cg','sag','saga'], #default=auto\n",
    "            #'random_state': grid_seed\n",
    "             #}],\n",
    "\n",
    "\n",
    "            #[{\n",
    "            #KNeighborsClassifier\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            #'n_neighbors': [1,2,3,4,5,6,7,8,9,10], #default: 5\n",
    "            #'weights': ['uniform', 'distance'], #default = ‘uniform’\n",
    "            #'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            #}],\n",
    "    \n",
    "    \n",
    "             [{\n",
    "            #LinearSVC\n",
    "            #https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "            #'lossstr' : [‘hinge’,‘squared_hinge’] (default=’squared_hinge’)\n",
    "            'fit_intercept' : grid_bool, #default=True   \n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "    \n",
    "            [{\n",
    "            #SVC\n",
    "            #http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n",
    "            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [1,2,3,4,5], #default=1.0\n",
    "            'gamma': grid_ratio, #default: auto\n",
    "            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "    \n",
    "            [{\n",
    "            #DiscriminantAnalysis\n",
    "            #https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
    "            'solver' : ['svd','lsqr','eigen'],  #default: svd\n",
    "            #'shrinkage' : ['auto', None], #default: none\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #XGBClassifier\n",
    "            #http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'learning_rate': grid_learn, #default: .3\n",
    "            'max_depth': [1,2,4,6,8,10], #default 2\n",
    "            'n_estimators': grid_n_estimator, \n",
    "            'seed': grid_seed  \n",
    "             }]   \n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tune \n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
    "for clf, param in zip (vote, grid_param): #https://docs.python.org/3/library/functions.html#zip\n",
    "    \n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
    "    best_search.fit(X_train, Y_train)\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "    print()\n",
    "    print()\n",
    "    print('-'*100)\n",
    "    print()\n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n",
    "print()\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "\n",
    "methods_otim = [\n",
    "    \n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(n_estimators = 300, learning_rate = .03, random_state = 0),\n",
    "    ensemble.BaggingClassifier(n_estimators = 300, bootstrap = True, max_samples = .25, random_state = 0),\n",
    "    ensemble.ExtraTreesClassifier(n_estimators = 10, bootstrap = False, criterion = 'entropy', max_depth = 8, random_state = 0),\n",
    "    ensemble.GradientBoostingClassifier(n_estimators = 50, loss = 'deviance', learning_rate = 0.25, max_depth = 2, random_state = 0),\n",
    "    ensemble.RandomForestClassifier(n_estimators = 100, criterion = 'gini', oob_score = True, max_depth = 10, \n",
    "                                    random_state = 0), \n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(fit_intercept = False, solver = 'saga', random_state = 0),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    #naive_bayes.BernoulliNB(alpha = 0.25),\n",
    "    \n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(n_neighbors = 10, weights = 'uniform', algorithm = 'ball_tree'),\n",
    "    \n",
    "    \n",
    "    #SVM\n",
    "    svm.LinearSVC(fit_intercept = False, random_state = 0),\n",
    "    svm.SVC(C = 2, gamma = .25, decision_function_shape = 'ovo', probability = True, random_state = 0),\n",
    "\n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(solver = 'eigen'),\n",
    "    \n",
    "    \n",
    "    #xgboost\n",
    "    XGBClassifier(n_estimators = 50, learning_rate = .03, max_samples = 4, random_state = 0)    \n",
    "\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "#split dataset in cross-validation with this splitter class\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "\n",
    "#cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0) \n",
    "\n",
    "\n",
    "#create table to compare parameters and metrics\n",
    "methods_columns = ['Method', 'Parameters', 'Test_Accuracy_Mean','Test_Accuracy_Std','Time']\n",
    "methods_compare = pd.DataFrame(columns = methods_columns)\n",
    "\n",
    "\n",
    "#create table to compare\n",
    "method_predict = pd.DataFrame(df_test['PassengerId'])\n",
    "\n",
    "\n",
    "#index through methods and save performance to table\n",
    "row_index = 0\n",
    "\n",
    "for method in methods_otim:\n",
    "\n",
    "    #set name and parameters\n",
    "    method_name = method.__class__.__name__\n",
    "    methods_compare.loc[row_index, 'Method'] = method_name\n",
    "    methods_compare.loc[row_index, 'Parameters'] = str(method.get_params())\n",
    "    \n",
    "    #score model with cross validation: \n",
    "    cv_results = model_selection.cross_validate(method, X_train, Y_train, cv  = cv_split)\n",
    "\n",
    "    methods_compare.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n",
    "    methods_compare.loc[row_index, 'Test_Accuracy_Mean'] = cv_results['test_score'].mean()\n",
    "    methods_compare.loc[row_index, 'Test_Accuracy_Std'] = cv_results['test_score'].std()  \n",
    "\n",
    "    method.fit(X_train, Y_train)\n",
    "    method_predict[method_name] = method.predict(X_test)\n",
    "    \n",
    "    row_index += 1\n",
    "\n",
    "    \n",
    "methods_compare.sort_values(by = ['Test_Accuracy_Mean'], ascending = False, inplace = True)\n",
    "methods_compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods_compare.to_csv('methods_compare_w_tuning.csv', index = False, encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gb = pd.concat([pd.DataFrame(df_test['PassengerId'])], axis=1)\n",
    "result_gb['Survived'] = method_predict['GradientBoostingClassifier']\n",
    "\n",
    "\n",
    "result_ext = pd.concat([pd.DataFrame(df_test['PassengerId'])], axis=1)\n",
    "result_ext['Survived'] = method_predict['ExtraTreesClassifier']\n",
    "\n",
    "\n",
    "result_svc = pd.concat([pd.DataFrame(df_test['PassengerId'])], axis=1)\n",
    "result_svc['Survived'] = method_predict['SVC']   \n",
    "\n",
    "\n",
    "result_rf = pd.concat([pd.DataFrame(df_test['PassengerId'])], axis=1)\n",
    "result_rf['Survived'] = method_predict['RandomForestClassifier']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gb.to_csv('result_gb_cv_wt.csv', index = False, encoding='utf-8')\n",
    "result_ext.to_csv('result_ext_cv_wt.csv', index = False, encoding='utf-8')\n",
    "result_svc.to_csv('result_svc_cv_wt.csv', index = False, encoding='utf-8')\n",
    "result_rf.to_csv('result_rf_cv_wt.csv', index = False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf[ result_rf['Survived'] != result_gb['Survived'] ].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
